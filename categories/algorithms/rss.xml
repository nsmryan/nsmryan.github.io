<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>itscomputersciencetime</title>
        <link>http%3A//nsmryan.github.com</link>
        <description></description>
        <generator>Zola</generator>
        <language>en</language>
        <atom:link href="http%3A//nsmryan.github.com/categories/algorithms/rss.xml" rel="self" type="application/rss+xml"/>
        <lastBuildDate>Sat, 23 Feb 2019 00:00:00 +0000</lastBuildDate>
        
            <item>
                <title>Floating Bar Numbers</title>
                <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
                <link>http%3A//nsmryan.github.com/floating-bar/</link>
                <guid>http%3A//nsmryan.github.com/floating-bar/</guid>
                <description>&lt;p&gt;There is an awesome power &lt;a href=&quot;https:&#x2F;&#x2F;iquilezles.org&#x2F;www&#x2F;articles&#x2F;floatingbar&#x2F;floatingbar.htm&quot;&gt;here&lt;&#x2F;a&gt; by Inigo Quilez
which develops his idea of a &amp;quot;floating bar&amp;quot; number. The core idea is to create a representation of rational
numbers that mimicks the &amp;quot;floating&amp;quot; property of floating points numbers.&lt;&#x2F;p&gt;
&lt;p&gt;Its an amazing article, and while he develops the concept throughly, more work on this seems like it could be
fruitful. The investigation for this article is related to graphics rendering, but I can&#x27;t help work happens
to other applications when floating point numbers are replaced by floating bar numbers. I imagine there
are certain applicatiosn out there where a floating bar could fit better then a floating point.&lt;&#x2F;p&gt;
&lt;p&gt;Anyway, I just wanted to put a link to this post on my blog because I love these alternative view of common
things, and I love thinking about the alternate world they imply.&lt;&#x2F;p&gt;
&lt;p&gt;Enjoy!&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>More Fun Algorithms</title>
                <pubDate>Sun, 11 Feb 2018 00:00:00 +0000</pubDate>
                <link>http%3A//nsmryan.github.com/more-fun-algorithm/</link>
                <guid>http%3A//nsmryan.github.com/more-fun-algorithm/</guid>
                <description>&lt;p&gt;This post is another gruop of fun algorithms (and data structures). These particular techniques are fun because they
are a core concept that can be applied to many different situations by simply changing some structure that the algorithm
is parameterized by. I won&#x27;t go into much detail here, but rather provide links to articles with more depth.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;guass-jordan-floyd-warshall-mcnaughton-yamada&quot;&gt;Guass-Jordan-Floyd-Warshall-McNaughton-Yamada&lt;&#x2F;h1&gt;
&lt;p&gt;The first one is the &lt;a href=&quot;http:&#x2F;&#x2F;r7.ca&#x2F;blog&#x2F;20110808T035622Z.html&quot;&gt;Guass-Jordan-Floyd-Warshall-McNaughton-Yamada&lt;&#x2F;a&gt; algorithm. This algorithm
solves a variety of problems, including finding shortest (max capcacity, most reliable, etc) pathes in a graph, finding an automata for a
regular expression, and solving linear equations. I like the linked article because it takes some bits of abstract algebra and frames these problems
in a general way, and then shows how you can see each problem as a special case of a single concept (asteration of a matrix). It also shows
how these positive ring structures appear to be common in computer science. They also appear in systems of algebraic data structures, so 
naturally I wonder if these algorithm can be used to solve any problems in that realm.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;finger-trees&quot;&gt;Finger Trees&lt;&#x2F;h1&gt;
&lt;p&gt;The next algorithm is the &lt;a href=&quot;http:&#x2F;&#x2F;www.staff.city.ac.uk&#x2F;%7Eross&#x2F;papers&#x2F;FingerTree.html&quot;&gt;fingertree&lt;&#x2F;a&gt;, which is parameterized by a monoid.
One introduction can be found &lt;a href=&quot;https:&#x2F;&#x2F;apfelmus.nfshost.com&#x2F;articles&#x2F;monoid-fingertree.html&quot;&gt;here&lt;&#x2F;a&gt;.
This data structure has good &lt;a href=&quot;https:&#x2F;&#x2F;abhiroop.github.io&#x2F;Finger-Trees&#x2F;&quot;&gt;asymptotics for a range of operations&lt;&#x2F;a&gt;, and can be used for a wide range of applications.
The implementation and uses are also described &lt;a href=&quot;http:&#x2F;&#x2F;andrew.gibiansky.com&#x2F;blog&#x2F;haskell&#x2F;finger-trees&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;. The thing I have used this structure for is simply as a
sequence structure that supports log(n) update to an index, and log(n) splitting and concatenation, but there is more to it then just that.&lt;&#x2F;p&gt;
&lt;p&gt;The core ability that this structure gives you is that it takes a computation that you would like to do over a data set, and performs your calculation incrementally.
This can be the calculation of indices, as when you use it as a sequence, but can also be for &lt;a href=&quot;http:&#x2F;&#x2F;blog.sigfpe.com&#x2F;2010&#x2F;11&#x2F;statistical-fingertrees.html&quot;&gt;statistics&lt;&#x2F;a&gt;
on data that is updated over time, without recalculating over the whole data set. It can be used to get constant time access and log time update to properties of your
tree, like its size, depth, the value of a predicate over its leaves, the greatest or least element (as in a priority queue), 
&lt;a href=&quot;http:&#x2F;&#x2F;blog.sigfpe.com&#x2F;2009&#x2F;01&#x2F;fast-incremental-regular-expression.html&quot;&gt;incremental regular expression matching&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;A couple other notes- there is a Haskell implementation &lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;fingertree&quot;&gt;here&lt;&#x2F;a&gt; for the general structure, and the specific use as a
random-access sequence &lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;fingertree-0.1.3.1&#x2F;docs&#x2F;Data-FingerTree.html&quot;&gt;here&lt;&#x2F;a&gt;. There is also a Haskell package implementing a
tree that accumulates both upwards (from the leaves) and downwards (from the root) found &lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;dual-tree&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;lenses&quot;&gt;Lenses&lt;&#x2F;h1&gt;
&lt;p&gt;The concept of a lens is a fascinating exploration into structure and computation, but there are plenty of resources on lenses, and I won&#x27;t be able to do it justice
here. The implementation &lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;lens&quot;&gt;here&lt;&#x2F;a&gt; is the main one to look at, although there 
&lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;lens-simple&quot;&gt;are&lt;&#x2F;a&gt; 
&lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;data-lens-light&quot;&gt;a&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;lenz&quot;&gt;number&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;microlens&quot;&gt;of&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;mezzolens&quot;&gt;others&lt;&#x2F;a&gt;, usually much simplier then the lens package. There are also implementations in other languages of course,
I&#x27;m just more familiar with Haskell. One particularly good introduction starts &lt;a href=&quot;https:&#x2F;&#x2F;artyom.me&#x2F;lens-over-tea-1&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To tie this into the common thread in this post, the properties of a lens depends on the choice of constraints on the type- in the type 
&lt;code&gt;Lens s t a b = forall f. (Functor f) =&amp;gt; (a -&amp;gt; f b) -&amp;gt; s -&amp;gt; f t&lt;&#x2F;code&gt; the type constructor &amp;quot;f&amp;quot; must be a functor, and this gives you a lens. If you constrain this type
with Applicative, you get a traversal, and so on.&lt;&#x2F;p&gt;
&lt;p&gt;You can even take this further and go up to the Optic type of this library, &lt;code&gt;Optic p f s t a b = p a (f b) -&amp;gt; p s (f t)&lt;&#x2F;code&gt;, and look at what structure you get with
a different profunctor. This can give you back lens when p is the function arrow, or Prisms with it is constrained by Choice, for example.
This can lead you to different universes of lens- I once used this to create lens that could pass data between each other, although I admit I abandended that approach as too complex.
This might be easier with profunctor lenses, I&#x27;m not sure.&lt;&#x2F;p&gt;
&lt;p&gt;I find this interesting because it seems like all of these universes of structures have their place, you just have to discover them.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;monads-from-types&quot;&gt;Monads From Types&lt;&#x2F;h1&gt;
&lt;p&gt;This deserves its own post, but this is another situation where you can get a lot of different systems out of a single concept. In this case, you can take many simple types,
and determine how they can form a monad, and it gets you a variety of forms of computation. For example, sum types give you the Either monad for computations that can fail,
the product type gives you the writer monad (requiring a monoid for one of the types), and the function arrow gives you the Reader monad. Any type (of kind * ) also gives you
the Identity Monad, trivially. One thing that is really cool here is to explore the duality between types, and then the duality between the forms of computation that they 
give rise to (sum vs product, monad vs comonad) creating a web of different concepts that also deserves its own post. Look at the Env comonad vs the Writer monad, the
traced comonad vs the Reader monad. For some reason there doesn&#x27;t appear to be a comonad for sum types. It seems like you need a constraint on the type argument in order
to implement extract, perhaps excluding them from normal use.&lt;&#x2F;p&gt;
&lt;p&gt;I like this because it shows one way in which data and computation are related- the building block of data structures each give a form of computation. This is common in Haskell
programming, where complex structures and algorithms come out of simple data types, set up just right. This reveals interesting subtlies in these definitions, where small variations
can have consequences in use, performance, or generality. One such example is shown &lt;a href=&quot;https:&#x2F;&#x2F;www.schoolofhaskell.com&#x2F;user&#x2F;edwardk&#x2F;moore&#x2F;for-less&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Interestingly, combinations of these can give you different forms of computation that are not the same as composing the resulting monads. Products and arrow gives you the state
monad, or the Store comonad, depending on the order you compose them. See &lt;a href=&quot;http:&#x2F;&#x2F;comonad.com&#x2F;reader&#x2F;2018&#x2F;the-state-comonad&#x2F;&quot;&gt;here&lt;&#x2F;a&gt; for more variations on this concept. This also
shows how sometimes contraints are necessary to get the correct structure, as in the Monoid constraint in the Writer monad, or the .&lt;&#x2F;p&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h1&gt;
&lt;p&gt;I think that is enough for now. These algorithms&#x2F;structures show how sometimes a choice of type or algebraic structure can either formulate a problem in a generic way, or
can give rise to a landscape of interesting objects, each with its own personality. I always enjoy seeing these different landscapes- there is something enjoyable about
learning that there are whole alternate universes of thought based on a different fundemental choice, each with different uses that bend ones mind to new worlds.&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>Some Fun Algorithms</title>
                <pubDate>Thu, 24 Nov 2016 00:00:00 +0000</pubDate>
                <link>http%3A//nsmryan.github.com/some-fun-algorithms/</link>
                <guid>http%3A//nsmryan.github.com/some-fun-algorithms/</guid>
                <description>&lt;p&gt;There are many interesting algorithms and data structures out there, but here are just some that I like.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hyperloglog- This is a algorithm for getting an approximate count for a large number of items in very little space.
It requires only a single pass through the data, which is useful for when not all elements can be held in memory at one time.&lt;&#x2F;p&gt;
&lt;p&gt;The paper is available &lt;a href=&quot;http:&#x2F;&#x2F;algo.inria.fr&#x2F;flajolet&#x2F;Publications&#x2F;FlFuGaMe07.pdf&quot;&gt;here&lt;&#x2F;a&gt; for a complete description,
and &lt;a href=&quot;http:&#x2F;&#x2F;blog.demofox.org&#x2F;2015&#x2F;03&#x2F;09&#x2F;hyperloglog-estimate-unique-value-counts-like-the-pros&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;. is a blog post
describing the algorithm.&lt;&#x2F;p&gt;
&lt;p&gt;There is also a &lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;hyperloglog&quot;&gt;Haskell implementation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Jump Flooding Algorithm- This is an algorithm for creating a discrete voronoi diagram on the GPU in log(n) time where n is the larger of the
width and height of the grid. This also works just as well in 3D, its just easier to talk about a grid then a 3D space.&lt;&#x2F;p&gt;
&lt;p&gt;The main idea is to propogate information about the nearest neighbor of a cell around the grid by having each cell look not at its
immediate neighbors, but at the cells n&#x2F;2 cells away, then n&#x2F;4, then n&#x2F;8, etc. This halving is what gives this algorithm its log time complexity.&lt;&#x2F;p&gt;
&lt;p&gt;The original &lt;a href=&quot;http:&#x2F;&#x2F;www.comp.nus.edu.sg&#x2F;%7Etants&#x2F;jfa&#x2F;i3d06.pdf&quot;&gt;paper&lt;&#x2F;a&gt; is good, as well as some useful [variations](http:&#x2F;&#x2F;www.comp.nus.edu.sg&#x2F;~tants&#x2F;jfa&#x2F;JFA-Variants.pdf].&lt;&#x2F;p&gt;
&lt;p&gt;I have also used it to produce a signed distance transform of an image in Unity, based on this &lt;a href=&quot;http:&#x2F;&#x2F;blog.demofox.org&#x2F;2016&#x2F;02&#x2F;29&#x2F;fast-voronoi-diagrams-and-distance-dield-textures-on-the-gpu-with-the-jump-flooding-algorithm&#x2F;&quot;&gt;blog post&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Hash Array Mapped Tree- This data structure provides some really nice asymptotics for a sequence data structure. The complexity of some operations is 
constant because the tree has a maximum depth, so it can only require so many operations to walk down in the worst case.
A &lt;a href=&quot;https:&#x2F;&#x2F;infoscience.epfl.ch&#x2F;record&#x2F;64398&#x2F;files&#x2F;idealhashtrees.pdf&quot;&gt;paper&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;There are other interesting sequence structures like Finger Trees and Relaxed Radix Balanced Trees.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Discrimination- Sorting in linear time, as well as other operations involving grouping objects.
The idea here is that you can perform these operations, sorting in particular, on generic data structures. I haven&#x27;t looked into much more
then the talk (by Edward Kmett), but he talks about applying radix sort and American flag sort to generic data, as well as a whole
diversion into a vocabulary of contravariant functors and other fun things.&lt;&#x2F;p&gt;
&lt;p&gt;The page of the &lt;a href=&quot;http:&#x2F;&#x2F;www.diku.dk&#x2F;hjemmesider&#x2F;ansatte&#x2F;henglein&#x2F;&quot;&gt;author&lt;&#x2F;a&gt;, the &lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;discrimination&quot;&gt;Haskell implementation&lt;&#x2F;a&gt;,
and a &lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cB8DapKQz-I&quot;&gt;talk&lt;&#x2F;a&gt; about the Haskell implementation.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Genetic Algorithms- I should mention these, since I have been interested in them since Grad school. I won&#x27;t go into detail here- they are
discussed in thousands of places. I just want to mention some of the interesting variations like Gene Expression Programming, Genetic Programming,
Population Based Incremental Learning, Grammatical Evolution, and Developmental Evolution. Every part of Genetic Algorithms has been investigated,
so there are a huge number of variations in operators, population structure, individual structure, etc.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Learning Classifier Systems- These are a very cool variation of Genetic Algorithms. They describe a system that takes input (say, from
a sensor), and determines an output by matching a series of templates against the input. The templates that match include an action to take,
as well as extra data depending on the variation of this algorithm such as the expected payoff of the action.&lt;&#x2F;p&gt;
&lt;p&gt;These systems are interesting because the templates are created by a Genetic Algorithm which is evolving an entire population that collectively
determines the system&#x27;s behavior. The contents of the matching templates, as well as the actions and some of the extra data, are the subject
of evolution.&lt;&#x2F;p&gt;
&lt;p&gt;The fun thing about this algorithm, which is really a whole family of algorithms), is that it takes Genetic Algorithms from an optimization
algorithm to something that reacts to an environment. I find this a fascinating transformation.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Condensed Probability Tables- This is a case where a technique that was not feasible when it was invented (due to memory constraints) is now
easily useable and very fast. It essentially precomputes a table used to sample from a certain discrete probability distribution.
You just give a series of elements and a weigh or probability, and you can get a constant time sampling from that distribution.&lt;&#x2F;p&gt;
&lt;p&gt;For an example, here is a &lt;a href=&quot;https:&#x2F;&#x2F;hackage.haskell.org&#x2F;package&#x2F;mwc-random-0.13.4.0&#x2F;docs&#x2F;System-Random-MWC-CondensedTable.html&quot;&gt;Haskell implementation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
    </channel>
</rss>
